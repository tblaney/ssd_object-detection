{"cells":[{"cell_type":"markdown","metadata":{"id":"eVMWVwHtIuug"},"source":["Import:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U57cwClyIAWR","outputId":"0580b13d-4567-4125-fd61-91eda33a8d3c","executionInfo":{"status":"ok","timestamp":1675470104351,"user_tz":480,"elapsed":17939,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Eo-kPXksGR7s","executionInfo":{"status":"ok","timestamp":1675470080182,"user_tz":480,"elapsed":4893,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","import random\n","\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","from PIL import Image\n","\n","import numpy as np\n","import os\n","import cv2\n","\n","import argparse\n","import time\n","\n","#import wandb\n","\n","from scipy.special import softmax\n","import  re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6mzwpR6TNE-"},"outputs":[],"source":["!unzip \"drive/MyDrive/machine-learning/ssd/data_ssd.zip\""]},{"cell_type":"code","source":["!pip install wandb\n","!wandb login 22ed476c0b9f3220c32f86c9de19e34fe91112cf"],"metadata":{"id":"PlpwzCQQ4jH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ai3joqBrJ55N","cellView":"form","executionInfo":{"status":"ok","timestamp":1675470834963,"user_tz":480,"elapsed":234,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["#@title Dataset Methods:\n","\n","#generate default bounding boxes\n","def default_box_generator(layers, large_scale, small_scale):\n","    #input:\n","    #layers      -- a list of sizes of the output layers. in this assignment, it is set to [10,5,3,1].\n","    #large_scale -- a list of sizes for the larger bounding boxes. in this assignment, it is set to [0.2,0.4,0.6,0.8].\n","    #small_scale -- a list of sizes for the smaller bounding boxes. in this assignment, it is set to [0.1,0.3,0.5,0.7].\n","    \n","    #output:\n","    #boxes -- default bounding boxes, shape=[box_num,8]. box_num=4*(10*10+5*5+3*3+1*1) for this assignment.\n","    \n","    #TODO:\n","    #create an numpy array \"boxes\" to store default bounding boxes\n","    #you can create an array with shape [10*10+5*5+3*3+1*1,4,8], and later reshape it to [box_num,8]\n","    #the first dimension means number of cells, 10*10+5*5+3*3+1*1\n","    #the second dimension 4 means each cell has 4 default bounding boxes.\n","    #their sizes are [ssize,ssize], [lsize,lsize], [lsize*sqrt(2),lsize/sqrt(2)], [lsize/sqrt(2),lsize*sqrt(2)],\n","    #where ssize is the corresponding size in \"small_scale\" and lsize is the corresponding size in \"large_scale\".\n","    #for a cell in layer[i], you should use ssize=small_scale[i] and lsize=large_scale[i].\n","    #the last dimension 8 means each default bounding box has 8 attributes: [x_center, y_center, box_width, box_height, x_min, y_min, x_max, y_max]\n","\n","    #IMPLEMENTATION:\n","    n_boxes = 0\n","    number_of_layer = len(layers)\n","    for grid in layers:\n","        n_boxes += grid * grid\n","\n","    # print(\"default box \")\n","    boxes = np.zeros([n_boxes, 4, 8])\n","    n = 0\n","    for i in range(0, number_of_layer):\n","        for x in range(0, layers[i]):\n","            for y in range(0, layers[i]):\n","                x_center = (x + 0.5) / layers[i]\n","                y_center = (y + 0.5) / layers[i]\n","                if layers[i] == 3:\n","                    x_center = round(x_center, 2)\n","                    y_center = round(y_center, 2)\n","                l_s_scale = round(large_scale[i] / 1.4, 2)\n","                boxes[n, 0] = [x_center, y_center, small_scale[i], small_scale[i], x_center - small_scale[i] / 2,\n","                                    y_center - small_scale[i] / 2, x_center + small_scale[i] / 2,\n","                                    y_center + small_scale[i] / 2]\n","\n","                boxes[n, 1] = [x_center, y_center, large_scale[i], large_scale[i], x_center - large_scale[i] / 2,\n","                                    y_center - large_scale[i] / 2, x_center + large_scale[i] / 2,\n","                                    y_center + large_scale[i] / 2]\n","\n","                boxes[n, 2] = [x_center, y_center, large_scale[i] * 1.4, l_s_scale,\n","                                    x_center - large_scale[i] * 1.4 / 2, y_center - l_s_scale / 2,\n","                                    x_center + large_scale[i] * 1.4 / 2, y_center + l_s_scale / 2]\n","\n","                boxes[n, 3] = [x_center, y_center, l_s_scale, large_scale[i] * 1.4, x_center - l_s_scale / 2,\n","                                    y_center - large_scale[i] * 1.4 / 2, x_center + l_s_scale / 2,\n","                                    y_center + large_scale[i] * 1.4 / 2]\n","                n += 1\n","\n","\n","    boxes = boxes.reshape((n_boxes*4, 8))\n","    boxes[boxes < 0] = 0\n","\n","    return boxes\n","\n","\n","#this is an example implementation of IOU.\n","#It is different from the one used in YOLO, please pay attention.\n","#you can define your own iou function if you are not used to the inputs of this one.\n","def iou(boxs_default, x_min,y_min,x_max,y_max):\n","    #input:\n","    #boxes -- [num_of_boxes, 8], a list of boxes stored as [box_1,box_2, ...], where box_1 = [x1_center, y1_center, width, height, x1_min, y1_min, x1_max, y1_max].\n","    #x_min,y_min,x_max,y_max -- another box (box_r)\n","    \n","    #output:\n","    #ious between the \"boxes\" and the \"another box\": [iou(box_1,box_r), iou(box_2,box_r), ...], shape = [num_of_boxes]\n","\n","    inter = np.maximum(np.minimum(boxs_default[:,6],x_max)-np.maximum(boxs_default[:,4],x_min),0)*np.maximum(np.minimum(boxs_default[:,7],y_max)-np.maximum(boxs_default[:,5],y_min),0)\n","    area_a = (boxs_default[:,6]-boxs_default[:,4])*(boxs_default[:,7]-boxs_default[:,5])\n","    area_b = (x_max-x_min)*(y_max-y_min)\n","    union = area_a + area_b - inter\n","    return inter/np.maximum(union,1e-8)\n","\n","\n","def match(ann_box,ann_confidence,boxs_default,threshold,cat_id,x_min,y_min,x_max,y_max, image):\n","    #input:\n","    #ann_box                 -- [num_of_boxes,4], ground truth bounding boxes to be updated\n","    #ann_confidence          -- [num_of_boxes,number_of_classes], ground truth class labels to be updated\n","    #boxs_default            -- [num_of_boxes,8], default bounding boxes\n","    #threshold               -- if a default bounding box and the ground truth bounding box have iou>threshold, then this default bounding box will be used as an anchor\n","    #cat_id                  -- class id, 0-cat, 1-dog, 2-person\n","    #x_min,y_min,x_max,y_max -- bounding box\n","    \n","    #compute iou between the default bounding boxes and the ground truth bounding box\n","    x_center = (x_min+x_max)/2\n","    y_center = (y_min+y_max)/2\n","    box_width = (x_max-x_min)\n","    box_height = (y_max-y_min)\n","\n","    ious = iou(boxs_default, x_min,y_min,x_max,y_max)\n","    ious_true = ious>threshold\n","\n","    #TODO:\n","    #update ann_box and ann_confidence, with respect to the ious and the default bounding boxes.\n","    #if a default bounding box and the ground truth bounding box have iou>threshold, then we will say this default bounding box is carrying an object.\n","    #this default bounding box will be used to update the corresponding entry in ann_box and ann_confidence\n","    \n","    true_idx = [index for (index,value) in enumerate(ious_true) if value == True]\n","    # print(true_idx)\n","    for i in true_idx:\n","        ann_confidence[i][3] = 0\n","        ann_confidence[i][cat_id] = 1\n","        ann_box[i][0] = (x_center-boxs_default[i][0])/boxs_default[i][2]\n","        ann_box[i][1] = (y_center-boxs_default[i][1])/boxs_default[i][3]\n","        ann_box[i][2] = np.log(box_width/boxs_default[i][2])\n","        ann_box[i][3] = np.log(box_height/boxs_default[i][3])\n","\n","    ious_true = np.argmax(ious)\n","    ann_confidence[ious_true][3] = 0\n","    ann_confidence[ious_true][cat_id] = 1\n","    ann_box[ious_true][0] = (x_center-boxs_default[ious_true][0])/boxs_default[ious_true][2]\n","    ann_box[ious_true][1] = (y_center-boxs_default[ious_true][1])/boxs_default[ious_true][3]\n","    ann_box[ious_true][2] = np.log(box_width/boxs_default[ious_true][2])\n","    ann_box[ious_true][3] = np.log(box_height/boxs_default[ious_true][3])\n","\n","\n","\n","\n","def random_crop(image,w_crop,h_crop):\n","\n","    x = random.randint(0, image.shape[1] - w_crop)\n","    y = random.randint(0, image.shape[0] - h_crop)\n","\n","    image = image[y:y+h_crop, x:x+w_crop]\n","    return x,y,image\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1zT9XFLUmHI-","cellView":"form","executionInfo":{"status":"ok","timestamp":1675470836398,"user_tz":480,"elapsed":216,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["#@title COCO Dataset:\n","\n","class COCO(torch.utils.data.Dataset):\n","    def __init__(self, imgdir, anndir, class_num, boxs_default, train = True, image_size=320, train_test_split = 0.9):\n","\n","        self.train = train\n","        self.imgdir = imgdir\n","        self.anndir = anndir\n","        self.class_num = class_num\n","        self.train_test_split = train_test_split\n","        \n","        #overlap threshold for deciding whether a bounding box carries an object or no\n","        self.threshold = 0.5\n","        self.boxs_default = boxs_default\n","        self.box_num = len(self.boxs_default)\n","        \n","        self.img_names = os.listdir(self.imgdir)\n","        self.image_size = image_size\n","        \n","        #notice:\n","        #you can split the dataset into 90% training and 10% validation here, by slicing self.img_names with respect to self.train\n","\n","    def __len__(self):\n","        # do training/test split here\n","        n_train = int(len(self.img_names)*self.train_test_split)\n","        if self.train:\n","            #print(\"Dataset get length train: \", str(len(self.img_names[0:n_train])))\n","            return len(self.img_names[0:n_train])\n","        else:\n","            #print(\"Dataset get length test: \", str(len(self.img_names[n_train:len(self.img_names)])))\n","            return len(self.img_names[n_train:len(self.img_names)])\n","\n","    def __getitem__(self, index):\n","\n","        ann_box = np.zeros([self.box_num,4], np.float32) #bounding boxes\n","        ann_confidence = np.zeros([self.box_num,self.class_num], np.float32) #one-hot vectors\n","        #one-hot vectors with four classes\n","        #[1,0,0,0] -> cat\n","        #[0,1,0,0] -> dog\n","        #[0,0,1,0] -> person\n","        #[0,0,0,1] -> background\n","        \n","        ann_confidence[:,-1] = 1 #the default class for all cells is set to \"background\"\n","        \n","        n_train = int(len(self.img_names)*self.train_test_split)\n","        images_train = self.img_names[0:n_train]\n","        images_test = self.img_names[n_train:len(self.img_names)]\n","\n","        if self.train:       \n","            img_name = self.imgdir+images_train[index]\n","            ann_name = self.anndir+images_train[index][:-3]+\"txt\"\n","        else:\n","            img_name = self.imgdir+images_test[index]\n","            ann_name = self.anndir+images_test[index][:-3]+\"txt\"\n","        \n","        #TODO:\n","        #1. prepare the image [3,320,320], by reading image \"img_name\" first.\n","        #2. prepare ann_box and ann_confidence, by reading txt file \"ann_name\" first.\n","        #3. use the above function \"match\" to update ann_box and ann_confidence, for each bounding box in \"ann_name\".\n","        #4. Data augmentation. You need to implement random cropping first. You can try adding other augmentations to get better results.\n","\n","        image = cv2.imread(img_name)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        # check for grayscale and correct\n","        if len(image.shape) == 2:\n","            image_fixed = np.zeros([image.shape[0], image.shape[1], 3])\n","            image_fixed[:,:,0] = image\n","            image_fixed[:,:,1] = image\n","            image_fixed[:,:,2] = image\n","            image = image_fixed\n","\n","        do_crop = False\n","\n","        height = image.shape[0]\n","        width = image.shape[1]\n","\n","        if do_crop and width > 320 and height > 320:\n","            crop_x, crop_y, image_ = random_crop(image, self.image_size, self.image_size)\n","            with open(ann_name, 'r') as f:\n","                for line in f:\n","                    class_id, x_min, y_min, box_width, box_height = line.split()\n","                    class_id = int(class_id)\n","                    x_min = float(x_min)\n","                    y_min = float(y_min)\n","                    box_width = float(box_width)\n","                    box_height = float(box_height)\n","\n","                    x_max = x_min + box_width\n","                    y_max = y_min + box_height\n","                    x_min = x_min - crop_x\n","                    y_min = y_min - crop_y\n","                    x_max = x_max - crop_x\n","                    y_max = y_max - crop_y\n","                    \n","                    x_min_normalized = round(x_min/320,2)\n","                    y_min_normalized = round(y_min/320,2)\n","                    x_max_normalized = round(x_max/ 320,2)\n","                    y_max_normalized= round(y_max/320,2)\n","                    \n","                    match(ann_box, ann_confidence, self.boxs_default, self.threshold, class_id, x_min_normalized, y_min_normalized, x_max_normalized, y_max_normalized, image_)\n","            \n","            image_ = cv2.resize(image_, (320, 320))\n","            image = torch.from_numpy(image_)\n","            image = image.type(torch.FloatTensor)\n","            image =  torch.permute(image, (2, 0, 1))\n","\n","            image = transforms.Resize([self.image_size,self.image_size])(image)\n","            ann_box = torch.from_numpy(ann_box)\n","            ann_confidence = torch.from_numpy(ann_confidence)\n","\n","            return image, ann_box, ann_confidence\n","        else:\n","            with open(ann_name, 'r') as f:\n","                for line in f:\n","                    class_id, x_min, y_min, box_width, box_height = line.split()\n","\n","                    class_id = int(class_id)\n","\n","                    x_min = float(x_min)\n","                    y_min = float(y_min)\n","                    box_width = float(box_width)\n","                    box_height = float(box_height)\n","\n","                    x_min_normalized = x_min/width\n","                    y_min_normalized= y_min/height\n","                    box_width_normalized= box_width/width\n","                    box_height_normalized = box_height/height\n","\n","                    x_max_normalized, y_max_normalized = x_min_normalized+box_width_normalized, y_min_normalized+box_height_normalized\n","                    \n","                    match(ann_box, ann_confidence, self.boxs_default, self.threshold, class_id, x_min_normalized, y_min_normalized, x_max_normalized, y_max_normalized, image)\n","        \n","        #to use function \"match\":\n","        #match(ann_box,ann_confidence,self.boxs_default,self.threshold,class_id,x_min,y_min,x_max,y_max)\n","        #where [x_min,y_min,x_max,y_max] is from the ground truth bounding box, normalized with respect to the width or height of the image.\n","\n","        #note: please make sure x_min,y_min,x_max,y_max are normalized with respect to the width or height of the image.\n","        #For example, point (x=100, y=200) in a image with (width=1000, height=500) will be normalized to (x/width=0.1,y/height=0.4)\n","        image = torch.from_numpy(image)\n","        image = image.type(torch.FloatTensor)\n","        image =  torch.permute(image, (2, 0, 1))\n","\n","        image = transforms.Resize([self.image_size,self.image_size])(image)\n","        ann_box = torch.from_numpy(ann_box)\n","        ann_confidence = torch.from_numpy(ann_confidence)\n","\n","        return image, ann_box, ann_confidence"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-x1upMpxNgBO","cellView":"form","executionInfo":{"status":"ok","timestamp":1675470838798,"user_tz":480,"elapsed":184,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["#@title Model:\n","\n","def SSD_loss(pred_confidence, pred_box, ann_confidence, ann_box):\n","    #input:\n","    #pred_confidence -- the predicted class labels from SSD, [batch_size, num_of_boxes, num_of_classes]\n","    #pred_box        -- the predicted bounding boxes from SSD, [batch_size, num_of_boxes, 4]\n","    #ann_confidence  -- the ground truth class labels, [batch_size, num_of_boxes, num_of_classes]\n","    #ann_box         -- the ground truth bounding boxes, [batch_size, num_of_boxes, 4]\n","    #\n","    #output:\n","    #loss -- a single number for the value of the loss function, [1]\n","\n","    #Note that you need to consider cells carrying objects and empty cells separately.\n","    #I suggest you to reshape confidence to [batch_size*num_of_boxes, num_of_classes]\n","    #and reshape box to [batch_size*num_of_boxes, 4].\n","    #Then you need to figure out how you can get the indices of all cells carrying objects,\n","    #and use confidence[indices], box[indices] to select those cells.\n","\n","    # reshape boxes\n","    batch_boxes = pred_confidence.shape[0]*pred_confidence.shape[1]\n","    pred_confidence = pred_confidence.reshape((batch_boxes, pred_confidence.shape[2]))\n","    pred_box = pred_box.reshape((batch_boxes, pred_box.shape[2]))\n","    ann_confidence = ann_confidence.reshape((batch_boxes, ann_confidence.shape[2]))\n","    ann_box = ann_box.reshape((batch_boxes, ann_box.shape[2]))\n","\n","    # get the indices of all cells carrying objects\n","    x_obj = []\n","    x_no_obj = []\n","    x_no_obj = ann_confidence[:,3]\n","    x_obj = 1-x_no_obj\n","\n","    no_obj = x_no_obj.cpu().numpy()\n","    obj = x_obj.cpu().numpy()\n","    x_no_ob = np.where(no_obj == 1)\n","    x_obj = np.where(obj == 1)\n","\n","    conf_pred_ob = pred_confidence[x_obj]\n","    conf_pred_noob = pred_confidence[x_no_ob]\n","    conf_ann_ob = ann_confidence[x_obj]\n","    conf_ann_noob = ann_confidence[x_no_ob]\n","    pred_box_ob = pred_box[x_obj]\n","    ann_box_ob = ann_box[x_obj]\n","\n","    L_cls = F.cross_entropy(conf_pred_ob, conf_ann_ob) + 3*F.cross_entropy(conf_pred_noob, conf_ann_noob)\n","    L_box = F.smooth_l1_loss(pred_box_ob, ann_box_ob)\n","    loss = (L_cls + L_box)\n","\n","    return loss\n","\n","\n","    #TODO: write a loss function for SSD\n","    #\n","    #For confidence (class labels), use cross entropy (F.cross_entropy)\n","    #You can try F.binary_cross_entropy and see which loss is better\n","    #For box (bounding boxes), use smooth L1 (F.smooth_l1_loss)\n","    #\n","    #Note that you need to consider cells carrying objects and empty cells separately.\n","    #I suggest you to reshape confidence to [batch_size*num_of_boxes, num_of_classes]\n","    #and reshape box to [batch_size*num_of_boxes, 4].\n","    #Then you need to figure out how you can get the indices of all cells carrying objects,\n","    #and use confidence[indices], box[indices] to select those cells.\n","\n","\n","def conv_base(in_channels, out_channels, kernelsize, stride):\n","    padding = (kernelsize-1) // 2\n","    x = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernelsize, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","    return x\n","\n","class SSD(nn.Module):\n","    def __init__(self, class_num):\n","        super(SSD, self).__init__()\n","        \n","        self.class_num = class_num #num_of_classes, in this assignment, 4: cat, dog, person, background\n","        self.conv1 = conv_base(3, 64, 3, 2)\n","        self.conv2 = conv_base(64, 64, 3, 1)\n","        self.conv3 = conv_base(64, 64, 3, 1)\n","        self.conv4 = conv_base(64, 128, 3, 2)\n","        self.conv5 = conv_base(128, 128, 3, 1)\n","        self.conv6 = conv_base(128, 128, 3, 1)\n","        self.conv7 = conv_base(128, 256, 3, 2)\n","        self.conv8 = conv_base(256, 256, 3, 1)\n","        self.conv9 = conv_base(256, 256, 3, 1)\n","        self.conv10 = conv_base(256, 512, 3, 2)\n","        self.conv11 = conv_base(512, 512, 3, 1)\n","        self.conv12 = conv_base(512, 512, 3, 1)\n","        self.conv13 = conv_base(512, 256, 3, 2)\n","        self.conv14 = conv_base(256, 256, 1, 1)\n","        self.conv15 = conv_base(256, 256, 3, 2)\n","        self.conv16 = conv_base(256, 256, 1, 1)\n","        self.conv17 = conv_base(256, 256, 3, 2)\n","        self.conv18 = conv_base(256, 256, 1, 1)\n","        self.conv19 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3,stride=3,padding=1),\n","            nn.ReLU()\n","        )\n","        self.conv_side1 = conv_base(256,16,3,1)\n","        self.conv_side2 = conv_base(256,16,3,1)\n","        self.conv_side3 = conv_base(256,16,3,1)\n","        self.conv_side4 = conv_base(256,16,3,1)\n","        self.conv_side5 = conv_base(256,16,3,1)\n","        self.conv_side6 = conv_base(256,16,3,1)\n","        self.conv_side7 = nn.Sequential(\n","            nn.Conv2d(256, 16, kernel_size=1,stride=1),\n","            nn.ReLU()\n","        )\n","        self.conv_side8 = nn.Sequential(\n","            nn.Conv2d(256, 16, kernel_size=1,stride=1),\n","            nn.ReLU()\n","        )\n","\n","        #TODO: define layers\n","        \n","        \n","    def forward(self, x):\n","        #input:\n","        #x -- images, [batch_size, 3, 320, 320]\n","        \n","        x = x/255.0 #normalize image. If you already normalized your input image in the dataloader, remove this line.\n","        \n","        #TODO: define forward\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.conv5(out)\n","        out = self.conv6(out)\n","        out = self.conv7(out)\n","        out = self.conv8(out)\n","        out = self.conv9(out)\n","        out = self.conv10(out)\n","        out = self.conv11(out)\n","        out = self.conv12(out)\n","        out = self.conv13(out)\n","        # 10 * 10\n","        right_1 = self.conv_side1(out)\n","        right_1 = right_1.reshape((right_1.shape[0],right_1.shape[1],right_1.shape[2]*right_1.shape[3]))\n","        left_1 = self.conv_side2(out)\n","        left_1 = left_1.reshape((left_1.shape[0],left_1.shape[1],left_1.shape[2]*left_1.shape[3]))\n","        out = self.conv14(out)\n","        out = self.conv15(out)\n","        # 5 * 5\n","        right_2 = self.conv_side3(out)\n","        right_2 = right_2.reshape((right_2.shape[0],right_2.shape[1],right_2.shape[2]*right_2.shape[3]))\n","        left_2 = self.conv_side4(out)\n","        left_2 = left_2.reshape((left_2.shape[0],left_2.shape[1],left_2.shape[2]*left_2.shape[3]))\n","        out = self.conv16(out)\n","        out = self.conv17(out)\n","        # 3 * 3\n","        right_3 = self.conv_side5(out)\n","        right_3 = right_3.reshape((right_3.shape[0],right_3.shape[1],right_3.shape[2]*right_3.shape[3]))\n","        left_3 = self.conv_side6(out)\n","        left_3 = left_3.reshape((left_3.shape[0],left_3.shape[1],left_3.shape[2]*left_3.shape[3]))\n","        out = self.conv18(out)\n","        out = self.conv19(out)\n","        # 1 * 1\n","        right_4 = self.conv_side7(out)\n","        right_4 = right_4.reshape((right_4.shape[0],right_4.shape[1],right_4.shape[2]*right_4.shape[3]))\n","        left_4 = self.conv_side8(out)\n","        left_4 = left_4.reshape((left_4.shape[0],left_4.shape[1],left_4.shape[2]*left_4.shape[3]))\n","\n","        #should you apply softmax to confidence? (search the pytorch tutorial for F.cross_entropy.) If yes, which dimension should you apply softmax?\n","        \n","        #sanity check: print the size/shape of the confidence and bboxes, make sure they are as follows:\n","        #confidence - [batch_size,4*(10*10+5*5+3*3+1*1),num_of_classes]\n","        #bboxes - [batch_size,4*(10*10+5*5+3*3+1*1),4]\n","\n","        # the concatenating order need to be the same as default bounding box\n","        confidence = torch.cat((left_1,left_2,left_3,left_4),2)\n","        bboxes = torch.cat((right_1,right_2,right_3,right_4),2)\n","        confidence =  torch.permute(confidence, (0, 2, 1))\n","        bboxes =  torch.permute(bboxes, (0, 2, 1))\n","        confidence = confidence.reshape((confidence.shape[0],540,self.class_num))\n","        bboxes = bboxes.reshape((bboxes.shape[0],540,4))\n","\n","        #confidence = F.softmax(confidence, dim = 2)\n","        m = nn.Softmax(dim=2)\n","        #confidence = F.softmax(confidence, dim = 2)\n","        confidence = m(confidence)\n","\n","        return confidence, bboxes\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dRzvI89gOiVD","cellView":"form","executionInfo":{"status":"ok","timestamp":1675470839561,"user_tz":480,"elapsed":13,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["#@title Utils:\n","\n","colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n","#use [blue green red] to represent different classes\n","\n","def visualize_pred(windowname, pred_confidence, pred_box, ann_confidence, ann_box, image_, boxs_default):\n","    #input:\n","    #windowname      -- the name of the window to display the images\n","    #pred_confidence -- the predicted class labels from SSD, [num_of_boxes, num_of_classes]\n","    #pred_box        -- the predicted bounding boxes from SSD, [num_of_boxes, 4]\n","    #ann_confidence  -- the ground truth class labels, [num_of_boxes, num_of_classes]\n","    #ann_box         -- the ground truth bounding boxes, [num_of_boxes, 4]\n","    #image_          -- the input image to the network\n","    #boxs_default    -- default bounding boxes, [num_of_boxes, 8]\n","    \n","    _, class_num = pred_confidence.shape\n","\n","    class_num = class_num-1\n","\n","    image = np.transpose(image_, (1,2,0)).astype(np.uint8)\n","\n","    image1 = np.zeros(image.shape,np.uint8)\n","    image2 = np.zeros(image.shape,np.uint8)\n","    image3 = np.zeros(image.shape,np.uint8)\n","    image4 = np.zeros(image.shape,np.uint8)\n","    # add two extra images for after NMS\n","    image5 = np.zeros(image.shape, np.uint8)\n","    image6 = np.zeros(image.shape, np.uint8)\n","\n","    image1[:]=image[:]\n","    image2[:]=image[:]\n","    image3[:]=image[:]\n","    image4[:]=image[:]\n","    image5[:]=image[:]\n","    image6[:]=image[:]\n","    #image1: draw ground truth bounding boxes on image1\n","    #image2: draw ground truth \"default\" boxes on image2 (to show that you have assigned the object to the correct cell/cells)\n","    #image3: draw network-predicted bounding boxes on image3\n","    #image4: draw network-predicted \"default\" boxes on image4 (to show which cell does your network think that contains an object)\n","\n","    #draw ground truth\n","    for i in range(len(ann_confidence)):\n","        for j in range(class_num):\n","\n","            if ann_confidence[i,j]>0.5: #if the network/ground_truth has high confidence on cell[i] with class[j]\n","                #TODO:\n","                #image1: draw ground truth bounding boxes on image1\n","                #image2: draw ground truth \"default\" boxes on image2 (to show that you have assigned the object to the correct cell/cells)\n","\n","                #you can use cv2.rectangle as follows:\n","                #start_point = (x1, y1) #top left corner, x1<x2, y1<y2\n","                #end_point = (x2, y2) #bottom right corner\n","                #color = colors[j] #use red green blue to represent different classes\n","                #thickness = 2\n","                #cv2.rectangle(image?, start_point, end_point, color, thickness)\n","\n","                min_gt, max_gt = get_box_shape(ann_box[i],boxs_default[i])\n","                min_default, max_default = get_minmax_box(boxs_default[i][0], boxs_default[i][1], boxs_default[i][2], boxs_default[i][3])\n","\n","                cv2.rectangle(image1, min_gt, max_gt, color=colors[j], thickness=2)\n","                cv2.rectangle(image2, min_default, max_default, color=colors[j], thickness=2)\n","\n","\n","    confidence_, box_, boxs_default_, index= [],[],[],[]\n","    color_index = []\n","    #pred\n","    for i in range(len(pred_confidence)):\n","        for j in range(class_num):\n","            if pred_confidence[i,j]>0.5:\n","                #TODO:\n","                #image3: draw network-predicted bounding boxes on image3\n","                #image4: draw network-predicted \"default\" boxes on image4 (to show which cell does your network think that contains an object)\n","                confidence_.append(pred_confidence[i,j])\n","                box_.append(pred_box[i])\n","                boxs_default_.append(boxs_default[i])\n","                index.append(i)\n","                color_index.append(j)\n","\n","                min_pred, max_pred = get_box_shape(pred_box[i], boxs_default[i])\n","                min_default, max_default = get_minmax_box(boxs_default[i][0], boxs_default[i][1],boxs_default[i][2], boxs_default[i][3])\n","\n","                cv2.rectangle(image3, min_pred, max_pred, color=colors[j], thickness=2)\n","                cv2.rectangle(image4, min_default, max_default, color=colors[j], thickness=2)\n","\n","\n","    results = non_maximum_suppression(confidence_, box_, boxs_default_, 0.2, 0.5)\n","\n","    for i in results:\n","        min_pred, max_pred = get_box_shape(pred_box[index[i]], boxs_default[index[i]])\n","        min_default, max_default = get_minmax_box(boxs_default[index[i]][0], boxs_default[index[i]][1], boxs_default[index[i]][2], boxs_default[index[i]][3])\n","\n","        cv2.rectangle(image5, min_pred, max_pred, color= colors[color_index[i]], thickness=2)\n","        cv2.rectangle(image6, min_default, max_default, color=colors[color_index[i]], thickness=2)\n","\n","\n","    h,w,_ = image1.shape\n","    image = np.zeros([h*3,w*2,3], np.uint8)\n","    image[:h,:w] = image1\n","    image[:h,w:] = image2\n","    image[h:2*h,:w] = image3\n","    image[h:2*h,w:] = image4\n","    image[2*h:,:w] = image5\n","    image[2*h:,w:] = image6\n","    \n","    ax=plt.gca()\n","    plt.imshow(image)\n","    ax.figure.set_size_inches(6, 6)\n","    #f, axarr = plt.subplots(3,2)\n","    #axarr[0,0].imshow(image1)\n","    #axarr[0,1].imshow(image2)\n","    #axarr[1,0].imshow(image3)\n","    #axarr[1,1].imshow(image4)\n","    #axarr[2,0].imshow(image5)\n","    #axarr[2,1].imshow(image6)\n","    plt.show()\n","\n","\n","\n","def non_maximum_suppression(confidence_, box_, boxs_default, overlap=0.1, threshold=0.5):\n","    #input:\n","    #confidence_  -- the predicted class labels from SSD, [num_of_boxes, num_of_classes]\n","    #box_         -- the predicted bounding boxes from SSD, [num_of_boxes, 4]\n","    #boxs_default -- default bounding boxes, [num_of_boxes, 8]\n","    #overlap      -- if two bounding boxes in the same class have iou > overlap, then one of the boxes must be suppressed\n","    #threshold    -- if one class in one cell has confidence > threshold, then consider this cell carrying a bounding box with this class.\n","    \n","    #output:\n","    #depends on your implementation.\n","    #if you wish to reuse the visualize_pred function above, you need to return a \"suppressed\" version of confidence [5,5, num_of_classes].\n","    #you can also directly return the final bounding boxes and classes, and write a new visualization function for that.\n","\n","    B = []\n","\n","    size = len(box_)\n","    confidence_ = np.array(confidence_)\n","\n","    for i in range(0, size):\n","        if confidence_.max() <= threshold:\n","            break\n","\n","        x = np.argmax(confidence_)\n","        B.append(x)\n","\n","        for j in range(0, size):\n","            if j!= x:\n","                if  confidence_[j] != 0:\n","                    start_point_max, end_point_max = get_box_shape(box_[x], boxs_default[x])\n","                    start_point_i, end_point_i = get_box_shape(box_[j],boxs_default[j])\n","                    inter = iou_ver2(start_point_max, end_point_max, start_point_i, end_point_i)\n","\n","                    if inter>overlap:\n","                        box_[j] = np.array([0,0,0,0])\n","                        boxs_default[j] = np.array([0,0,0,0])\n","                        confidence_[j] = 0\n","\n","        box_[x] = np.array([0,0,0,0])\n","        confidence_[x] = 0\n","\n","    return B\n","\n","\n","def get_box_shape(offset, box_default):\n","    # do reverse transformation as whats in match\n","    centre_x = offset[0]*box_default[2]+box_default[0]\n","    centre_y = offset[1]*box_default[3]+box_default[1]\n","    w = np.exp(offset[2])*box_default[2]\n","    h = np.exp(offset[3])*box_default[3]\n","\n","    start_point, end_point = get_minmax_box(centre_x,centre_y,w,h)\n","\n","    return start_point,end_point\n","\n","def get_minmax_box(x_c, y_c, w, h): \n","\n","    min_point = (max(int((x_c - w / 2)*320),10), max(int((y_c - h / 2)*320),10))\n","    max_point = (min(int((x_c + w / 2)*320),300), min(int((y_c + h / 2)*320),300))\n","\n","    return min_point, max_point  \n","\n","def iou_ver2(min_1, max_1, min_2, max_2):\n","\n","    inter = (np.minimum(max_2[0],max_1[0])-np.maximum(min_1[0],min_2[0])) * (np.minimum(max_2[1],max_1[1])-np.maximum(min_1[1],min_2[1]))\n","\n","    if inter < 0:\n","        inter = 0\n","\n","    area_a = (max_2[0] - min_2[0]) * (max_2[1] - min_2[1])\n","    area_b = (max_1[0] - min_1[0]) * (max_1[1] - min_1[1])\n","    \n","    union = area_a + area_b - inter\n","    return inter / np.maximum(union, 1e-8)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Sa2uysc3O4hj","executionInfo":{"status":"ok","timestamp":1675470840262,"user_tz":480,"elapsed":254,"user":{"displayName":"pip rock","userId":"07519269976867556045"}}},"outputs":[],"source":["#@title Eval Utils:\n","def update_precision_recall(identifier, pred_confidence_, pred_box_, ann_confidence_, ann_box_, boxs_default, thres, all_pred_boxes, all_true_boxes):\n","\n","    batch_size = ann_confidence_.shape[0]\n","    for idx in range(batch_size):\n","      identifier+=1\n","      pred_confidence = pred_confidence_[idx]\n","      pred_box = pred_box_[idx]\n","      ann_confidence = ann_confidence_[idx]\n","      ann_box = ann_box_[idx]\n","\n","      #pred_confidence = softmax(pred_confidence,axis=1)\n","\n","      for i in range(len(ann_confidence)):\n","          for j in range(0, 3):\n","              if ann_confidence[i,j]>thres:\n","                  start_point, end_point = get_box_shape(ann_box[i], boxs_default[i])\n","\n","                  all_true_boxes.append([identifier, j, ann_confidence[i,j], start_point, end_point])\n","\n","      confidence_, box_, boxs_default_, index, class_box = [],[],[],[],[]\n","      for i in range(len(pred_confidence)):\n","          for j in range(0, 3):\n","              if pred_confidence[i,j]>thres:\n","                  confidence_.append(pred_confidence[i,j])\n","                  box_.append(pred_box[i])\n","                  boxs_default_.append(boxs_default[i])\n","                  index.append(i)\n","                  class_box.append(j)\n","\n","\n","      results = non_maximum_suppression(confidence_,box_,boxs_default_,0.5,0.5)\n","      for i in results:\n","          start_point, end_point = get_box_shape(pred_box[index[i]], boxs_default[index[i]])\n","\n","          all_pred_boxes.append([identifier, class_box[i], pred_confidence[index[i], class_box[i]], start_point, end_point])\n","\n","    return all_pred_boxes, all_true_boxes, identifier\n","\n","\n","def update_precision_recall_test(identifier, pred_confidence_, pred_box_, ann_confidence_, ann_box_, boxs_default, thres, all_pred_boxes, all_true_boxes):\n","\n","    identifier+=1\n","\n","    pred_confidence = pred_confidence_\n","    pred_box = pred_box_\n","    ann_confidence = ann_confidence_[0]\n","    ann_box = ann_box_[0]\n","\n","    for i in range(len(ann_confidence)):\n","        for j in range(0, 3):\n","            if ann_confidence[i][j]>thres:\n","                start_point, end_point = get_box_shape(ann_box[i], boxs_default[i])\n","\n","                all_true_boxes.append([identifier, j, ann_confidence[i,j], start_point, end_point])\n","\n","    confidence_, box_, boxs_default_, index, class_box = [],[],[],[], []\n","    for i in range(len(pred_confidence)):\n","        for j in range(0, 3):\n","            if pred_confidence[i][j]>thres:\n","                confidence_.append(pred_confidence[i,j])\n","                box_.append(pred_box[i])\n","                boxs_default_.append(boxs_default[i])\n","                index.append(i)\n","                class_box.append(j)\n","\n","    results = non_maximum_suppression(confidence_,box_,boxs_default_,0.5,0.5)\n","    for i in results:\n","        start_point, end_point = get_box_shape(pred_box[index[i]], boxs_default[index[i]])\n","        all_pred_boxes.append([identifier, class_box[i], pred_confidence[index[i], class_box[i]], start_point, end_point])\n","\n","    return all_pred_boxes, all_true_boxes, identifier\n","\n","\n","\n","\n","def generate_mAP(pred_boxes_all, true_boxes_all, iou_threshold=0.5):\n","    #TODO: Generate mAP\n","\n","    average_precisions = []\n","\n","    precisions_all = []\n","    recalls_all = []\n","\n","    epsilon = 1e-6\n","\n","    for detection in pred_boxes_all:\n","        if detection[1] == 4:\n","            detections.append(detection)\n","\n","    for c in range(0,3):\n","        detections = []\n","        ground_truths = []\n","\n","        for detection in pred_boxes_all:\n","            if detection[1] == c:\n","                detections.append(detection)\n","\n","        for true_box in true_boxes_all:\n","            if true_box[1] == c:\n","                ground_truths.append(true_box)\n","\n","        # img 1 has 5 then we will obtain a dictionary with:\n","        # amount_bboxes = {0:3, 1:5}\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","        print(\"Generate mAP amount bboxes:\", str(amount_bboxes[0]))\n","\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # sort by box probabilities which is index 2\n","        detections.sort(key=lambda x: x[2], reverse=True)\n","        print(\"Generate mAP detections: \" + str(len(detections)))\n","        TP = torch.zeros((len(detections)))\n","        FP = torch.zeros((len(detections)))\n","        total_true_bboxes = len(ground_truths)\n","        print(\"Generate mAP gts: \" + str(len(ground_truths)))\n","\n","        # If none exists for this class then we can safely skip\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        for detection_idx, detection in enumerate(detections):\n","            # same image idx as detection\n","            ground_truth_img = [\n","                bbox for bbox in ground_truths if bbox[0] == detection[0]\n","            ]\n","\n","            num_gts = len(ground_truth_img)\n","            best_iou = 0\n","\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = iou_ver2(gt[3],gt[4],detection[3],detection[4])\n","\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            if best_iou > iou_threshold:\n","                # only detect ground truth detection once\n","                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                    # true positive and add this bounding box to seen\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","\n","            # false positive\n","            else:\n","                FP[detection_idx] = 1\n","\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","        # torch.trapz for numerical integration\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","        precisions_all.append(precisions.cpu().detach().numpy())\n","        recalls_all.append(recalls.cpu().detach().numpy())\n","\n","    mAP = sum(average_precisions) / len(average_precisions)\n","    return mAP, precisions_all, recalls_all"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcT2X761SX_L","cellView":"form"},"outputs":[],"source":["#@title Training:\n","\n","\n","class_num = 4 #cat dog person background\n","\n","num_epochs = 100\n","batch_size = 32\n","\n","boxs_default = default_box_generator([10,5,3,1], [0.2,0.4,0.6,0.8], [0.1,0.3,0.5,0.7])\n","\n","#Create network\n","network = SSD(class_num)\n","network.cuda()\n","cudnn.benchmark = True\n","\n","dataset = COCO(\"data/train/images/\", \"data/train/annotations/\", class_num, boxs_default, train = True, image_size=320)\n","dataset_test = COCO(\"data/train/images/\", \"data/train/annotations/\", class_num, boxs_default, train = False, image_size=320)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","optimizer = optim.Adam(network.parameters(), lr = 1e-4)\n","#feel free to try other optimizers and parameters.\n","#!wandb login 22ed476c0b9f3220c32f86c9de19e34fe91112cf\n","#!wandb login 22ed476c0b9f3220c32f86c9de19e34fe91112cf --relogin\n","#wandb.init()\n","#wandb.login(key=\"22ed476c0b9f3220c32f86c9de19e34fe91112cf\")\n","#wandb.init()\n","#wandb.config = {\n","#  \"learning_rate\": 1e-4,\n","#  \"epochs\": 100,\n","#  \"batch_size\": 32\n","#}\n","#wandb.watch(network)\n","\n","precision_ = []\n","recall_ = []\n","\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    #TRAINING\n","    network.train()\n","\n","    avg_loss = 0\n","    avg_count = 0\n","    for i, data in enumerate(dataloader, 0):\n","        images_, ann_box_, ann_confidence_ = data\n","        images = images_.cuda()\n","        ann_box = ann_box_.cuda()\n","        ann_confidence = ann_confidence_.cuda()\n","\n","        optimizer.zero_grad()\n","        pred_confidence, pred_box = network(images)\n","\n","        loss_net = SSD_loss(pred_confidence, pred_box, ann_confidence, ann_box)\n","        loss_net.backward()\n","        optimizer.step()\n","        \n","        avg_loss += loss_net.data\n","        avg_count += 1\n","\n","    print('Epoch Log: [%d] time: %f train loss: %f' % (epoch, time.time()-start_time, avg_loss/avg_count))\n","\n","    #wandb.log({\"loss\": avg_loss/avg_count})\n","    #visualize\n","    pred_confidence_ = pred_confidence[0].detach().cpu().numpy()\n","    pred_box_ = pred_box[0].detach().cpu().numpy()\n","    visualize_pred(\"train\", pred_confidence_, pred_box_, ann_confidence_[0].numpy(), ann_box_[0].numpy(), images_[0].numpy(), boxs_default)\n","    \n","    #VALIDATION\n","    network.eval()\n","    \n","    # TODO: split the dataset into 90% training and 10% validation\n","    # use the training set to train and the validation set to evaluate\n","\n","    pred_boxes_all = []\n","    ann_boxes_all = []\n","    avg_loss_val = 0\n","    avg_count_val = 0\n","    identifier = 0\n","    \n","    for i, data in enumerate(dataloader_test, 0):\n","        images_, ann_box_, ann_confidence_ = data\n","        images = images_.cuda()\n","        ann_box = ann_box_.cuda()\n","        ann_confidence = ann_confidence_.cuda()\n","\n","        pred_confidence, pred_box = network(images)\n","        \n","        pred_confidence_ = pred_confidence.detach().cpu().numpy()\n","        pred_box_ = pred_box.detach().cpu().numpy()\n","\n","        loss_net = SSD_loss(pred_confidence, pred_box, ann_confidence, ann_box)\n","        loss_net.backward()\n","        \n","        avg_loss_val += loss_net.data\n","        avg_count_val += 1\n","\n","        # need precision and recall:\n","\n","        #optional: implement a function to accumulate precision and recall to compute mAP or F1.\n","        #pred_boxes_all, ann_boxes_all, identifier = update_precision_recall(identifier, pred_confidence_, pred_box_, ann_confidence_.numpy(), ann_box_.numpy(), boxs_default, 0.5, pred_boxes_all, ann_boxes_all)\n","    \n","    #visualize\n","    pred_confidence_ = pred_confidence[0].detach().cpu().numpy()\n","    pred_box_ = pred_box[0].detach().cpu().numpy()\n","    visualize_pred(\"val\", pred_confidence_, pred_box_, ann_confidence_[0].numpy(), ann_box_[0].numpy(), images_[0].numpy(), boxs_default)\n","    \n","    print('Epoch Val Log: [%d] validation loss: %f' % (epoch, avg_loss_val/avg_count_val))\n","\n","    #optional: compute F1 \n","    #F1score = 2*precision*recall/np.maximum(precision+recall,1e-8)\n","    #print(F1score)\n","    #mAP = generate_mAP(pred_boxes_all, ann_boxes_all, 0.5)\n","    #print('Precisions: ', precisions.shape)\n","    #print('Recalls: ', recalls.shape)\n","    #print('Epoch Validation mAP: [%d] mAP: %f' % (epoch, mAP))\n","    \n","    #save weights\n","    if epoch%10==0:\n","        #save last network\n","        print('saving net...')\n","        torch.save(network.state_dict(), 'drive/MyDrive/machine-learning/network' + str(epoch) + '.pth')\n"]},{"cell_type":"code","source":["torch.save(network.state_dict(), 'drive/MyDrive/machine-learning/network' + str(100) + '.pth')"],"metadata":{"id":"v3v7HDxBGpne"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ibUpSVznSnZG","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11BsAYGLCybTkkBp4k3BzaDMMeR_TgLDD"},"executionInfo":{"status":"ok","timestamp":1675470989048,"user_tz":480,"elapsed":145259,"user":{"displayName":"pip rock","userId":"07519269976867556045"}},"outputId":"a40f09a4-a46e-4683-c3d3-c74114f6e03a","cellView":"form"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#@title Testing:\n","\n","class_num = 4 #cat dog person background\n","\n","num_epochs = 100\n","batch_size = 32\n","\n","boxs_default = default_box_generator([10,5,3,1], [0.2,0.4,0.6,0.8], [0.1,0.3,0.5,0.7])\n","\n","network = SSD(class_num)\n","network.cuda()\n","\n","dataset_test = COCO(\"data/validate/images/\", \"data/validate/annotations/\", class_num, boxs_default, train = True, image_size=320, train_test_split = 1)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n","network.load_state_dict(torch.load('drive/MyDrive/machine-learning/ssd/network100.pth'))\n","network.eval()\n","\n","pred_boxes_all = []\n","ann_boxes_all = []\n","identifier = 0\n","\n","for i, data in enumerate(dataloader_test, 0):\n","    images_, ann_box_, ann_confidence_ = data\n","    images = images_.cuda()\n","    ann_box = ann_box_.cuda()\n","    ann_confidence = ann_confidence_.cuda()\n","\n","    pred_confidence, pred_box = network(images)\n","\n","    pred_confidence_ = pred_confidence[0].detach().cpu().numpy()\n","    pred_box_ = pred_box[0].detach().cpu().numpy()\n","\n","    #print(pred_confidence_)\n","    \n","    pred_boxes_all, ann_boxes_all, identifier = update_precision_recall_test(identifier, pred_confidence_, pred_box_, ann_confidence_.numpy(), ann_box_.numpy(), boxs_default, 0.5, pred_boxes_all, ann_boxes_all)\n","\n","    #pred_confidence_, pred_box_ = non_maximum_suppression(pred_confidence_,pred_box_,boxs_default)\n","    \n","    #TODO: save predicted bounding boxes and classes to a txt file.\n","    #you will need to submit those files for grading this assignment\n","    with open('drive/MyDrive/machine-learning/ssd_test_log.txt', 'w') as f:\n","        f.writelines(str(pred_boxes_all))\n","    \n","    visualize_pred(\"test\", pred_confidence_, pred_box_, ann_confidence_[0].numpy(), ann_box_[0].numpy(), images_[0].numpy(), boxs_default)\n","    #cv2.waitKey(1000)\n","\n","mAP, precisions_all, recalls_all = generate_mAP(pred_boxes_all, ann_boxes_all, 0.5)\n","print('Validation mAP: mAP: %f' % (mAP))\n","\n","\n","for i in range(len(precisions_all)):\n","  fig = plt.figure()\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.plot(recalls_all[i], precisions_all[i], color ='tab:blue')\n","    \n","  # set the limits\n","  ax.set_title('precisions recall for class: ' + str(i))\n","  \n","  # display the plot\n","  plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}